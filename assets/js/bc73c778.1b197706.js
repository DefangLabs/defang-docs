"use strict";(self.webpackChunkdefang_docs=self.webpackChunkdefang_docs||[]).push([[3],{28453:(e,n,o)=>{o.d(n,{R:()=>a,x:()=>c});var i=o(96540);const t={},s=i.createContext(t);function a(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),i.createElement(s.Provider,{value:n},e.children)}},33248:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>r,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>c,toc:()=>l});var i=o(74848),t=o(28453);const s={title:"Deploying your OpenAI application to AWS using Bedrock",sidebar_position:50},a="Deploying your OpenAI application to AWS using Bedrock",c={id:"tutorials/deploying-openai-apps-aws-bedrock",title:"Deploying your OpenAI application to AWS using Bedrock",description:"Let's assume you have an app which is using one of the OpenAI client libraries and you want to deploy your app to AWS so you can leverage Bedrock. This tutorial will show you how Defang makes it easy.",source:"@site/docs/tutorials/deploying-openai-apps-aws-bedrock.mdx",sourceDirName:"tutorials",slug:"/tutorials/deploying-openai-apps-aws-bedrock",permalink:"/docs/tutorials/deploying-openai-apps-aws-bedrock",draft:!1,unlisted:!1,editUrl:"https://github.com/DefangLabs/defang-docs/tree/main/docs/tutorials/deploying-openai-apps-aws-bedrock.mdx",tags:[],version:"current",sidebarPosition:50,frontMatter:{title:"Deploying your OpenAI application to AWS using Bedrock",sidebar_position:50},sidebar:"docsSidebar",previous:{title:"Deploy to Playground",permalink:"/docs/tutorials/deploy-to-playground"},next:{title:"Generate Project Outlines With AI",permalink:"/docs/tutorials/generate-new-code-using-ai"}},r={},l=[{value:"Add an LLM service to your compose file",id:"add-an-llm-service-to-your-compose-file",level:2},{value:"Redirecting application traffic",id:"redirecting-application-traffic",level:2},{value:"Selecting a model",id:"selecting-a-model",level:2},{value:"Complete Example Compose File",id:"complete-example-compose-file",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",p:"p",pre:"pre",strong:"strong",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"deploying-your-openai-application-to-aws-using-bedrock",children:"Deploying your OpenAI application to AWS using Bedrock"}),"\n",(0,i.jsx)(n.p,{children:"Let's assume you have an app which is using one of the OpenAI client libraries and you want to deploy your app to AWS so you can leverage Bedrock. This tutorial will show you how Defang makes it easy."}),"\n",(0,i.jsx)(n.p,{children:"Assume you have a compose file like this:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'services:\n  app:\n    build:\n        context: .\n    ports:\n      - 3000:3000\n    environment:\n      OPENAI_API_KEY:\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:3000/"]\n'})}),"\n",(0,i.jsx)(n.h2,{id:"add-an-llm-service-to-your-compose-file",children:"Add an LLM service to your compose file"}),"\n",(0,i.jsxs)(n.p,{children:["The first step is to add a new service to your compose file: the ",(0,i.jsx)(n.code,{children:"defangio/openai-access-gateway"}),". This service provides an OpenAI compatible interface to AWS Bedrock. It's easy to configure, first you need to add it to your compose file:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-diff",children:"+  llm:\n+    image: defangio/openai-access-gateway\n+    x-defang-llm: true\n+    ports:\n+      - target: 80\n+        published: 80\n+        mode: host\n+    environment:\n+      - OPENAI_API_KEY\n"})}),"\n",(0,i.jsxs)(n.p,{children:["A few things to note here. First the image is a fork of ",(0,i.jsx)(n.a,{href:"https://github.com/aws-samples/bedrock-access-gateway",children:"aws-samples/bedrock-access-gateway"}),", with a few modifications to make it easier to use. The source code is available ",(0,i.jsx)(n.a,{href:"https://github.com/DefangLabs/openai-access-gateway",children:"here"}),". Second: the ",(0,i.jsx)(n.code,{children:"x-defang-llm"})," property. Defang uses extensions like this to signal special handling of certain kinds of services. In this case, it signals to Defang that we need to configure the appropriate IAM Roles and Policies to support your application."]}),"\n",(0,i.jsxs)(n.admonition,{type:"warning",children:[(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Your OpenAI key"})}),(0,i.jsxs)(n.p,{children:["You no longer need to use your original OpenAI API key. We do recommend using ",(0,i.jsx)(n.em,{children:"something"})," in its place, but feel free to generate a new secret and set it with ",(0,i.jsx)(n.code,{children:"defang config set OPENAI_API_KEY --random"}),"."]}),(0,i.jsx)(n.p,{children:"This is used to authenticate your application service with the openai-access-gateway."})]}),"\n",(0,i.jsx)(n.h2,{id:"redirecting-application-traffic",children:"Redirecting application traffic"}),"\n",(0,i.jsx)(n.p,{children:"Then you need to configure your application to redirect traffic to the openai-access-gateway, like this:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-diff",children:' services:\n   app:\n     ports:\n       - 3000:3000\n     environment:\n       OPENAI_API_KEY:\n+      OPENAI_BASE_URL: "http://llm/api/v1"\n     healthcheck:\n       test: ["CMD", "curl", "-f", "http://localhost:3000/"]\n'})}),"\n",(0,i.jsx)(n.h2,{id:"selecting-a-model",children:"Selecting a model"}),"\n",(0,i.jsxs)(n.p,{children:["You will also need to configure your application to use one of the bedrock models. We recommend configuring an environment variable called ",(0,i.jsx)(n.code,{children:"MODEL"})," like this:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-diff",children:' services:\n   app:\n     ports:\n       - 3000:3000\n     environment:\n       OPENAI_API_KEY:\n       OPENAI_BASE_URL: "http://llm/api/v1"\n+      MODEL: "anthropic.claude-3-sonnet-20240229-v1:0"\n     healthcheck:\n       test: ["CMD", "curl", "-f", "http://localhost:3000/"]\n'})}),"\n",(0,i.jsxs)(n.admonition,{type:"warning",children:[(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Enabling bedrock model access"})}),(0,i.jsxs)(n.p,{children:["AWS currently requires access to be manually configured on a per-model basis in each account. See this guide for ",(0,i.jsx)(n.a,{href:"https://docs.aws.amazon.com/bedrock/latest/userguide/model-access-modify.html",children:"how to enable model access"}),"."]})]}),"\n",(0,i.jsx)(n.h2,{id:"complete-example-compose-file",children:"Complete Example Compose File"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'services:\n  app:\n    build:\n        context: .\n    ports:\n      - 3000:3000\n    environment:\n      OPENAI_API_KEY:\n      OPENAI_BASE_URL: "http://llm/api/v1"\n      MODEL: "us:anthropic.claude-3-sonnet-20240229-v1:0"\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:3000/"]\n  llm:\n    image: defangio/openai-access-gateway\n    x-defang-llm: true\n    ports:\n      - target: 80\n        published: 80\n        mode: host\n    environment:\n      - OPENAI_API_KEY\n\n'})})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);