"use strict";(self.webpackChunkdefang_docs=self.webpackChunkdefang_docs||[]).push([[7385],{28453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>c});var a=s(96540);const o={},t=a.createContext(o);function r(e){const n=a.useContext(t);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),a.createElement(t.Provider,{value:n},e.children)}},48307:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>d,contentTitle:()=>r,default:()=>p,frontMatter:()=>t,metadata:()=>c,toc:()=>i});var a=s(74848),o=s(28453);const t={title:"OpenAI Access Gateway",description:"Defang makes it easy to leverage cloud-native managed language models for your OpenAI-compatible application.",sidebar_position:3e3},r="OpenAI Access Gateway",c={id:"concepts/managed-llms/openai-access-gateway",title:"OpenAI Access Gateway",description:"Defang makes it easy to leverage cloud-native managed language models for your OpenAI-compatible application.",source:"@site/docs/concepts/managed-llms/openai-access-gateway.md",sourceDirName:"concepts/managed-llms",slug:"/concepts/managed-llms/openai-access-gateway",permalink:"/docs/concepts/managed-llms/openai-access-gateway",draft:!1,unlisted:!1,editUrl:"https://github.com/DefangLabs/defang-docs/tree/main/docs/concepts/managed-llms/openai-access-gateway.md",tags:[],version:"current",sidebarPosition:3e3,frontMatter:{title:"OpenAI Access Gateway",description:"Defang makes it easy to leverage cloud-native managed language models for your OpenAI-compatible application.",sidebar_position:3e3},sidebar:"docsSidebar",previous:{title:"Using Managed LLMs",permalink:"/docs/concepts/managed-llms/managed-language-models"},next:{title:"Managed Storage",permalink:"/docs/concepts/managed-storage/"}},d={},i=[{value:"Docker Provider Services",id:"docker-provider-services",level:2},{value:"Current Support",id:"current-support",level:2}];function l(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"openai-access-gateway",children:"OpenAI Access Gateway"}),"\n",(0,a.jsxs)(n.p,{children:["Defang makes it easy to deploy on your favourite cloud's managed LLM service with our ",(0,a.jsx)(n.a,{href:"https://github.com/DefangLabs/openai-access-gateway",children:"OpenAI Access Gateway"}),". This service sits between your application and the cloud service and acts as a compatibility layer.\nIt handles incoming OpenAI requests, translates those requests to the appropriate cloud-native API, handles the native response, and re-constructs an OpenAI-compatible response."]}),"\n",(0,a.jsxs)(n.p,{children:["See ",(0,a.jsx)(n.a,{href:"/docs/tutorials/deploying-openai-apps-aws-bedrock-gcp-vertex/",children:"our tutorial"})," which describes how to configure the OpenAI Access Gateway for your application"]}),"\n",(0,a.jsx)(n.h2,{id:"docker-provider-services",children:"Docker Provider Services"}),"\n",(0,a.jsxs)(n.p,{children:["As of Docker Compose v2.35 and Docker Desktop v4.41, Compose introduces a new service type called ",(0,a.jsx)(n.code,{children:"provider"})," that allows you to declare platform capabilities required by your application.\nFor AI models, you use the ",(0,a.jsx)(n.code,{children:"model"})," type to declare model dependencies. This will expose an OpenAI compatible API for your service. Check the ",(0,a.jsx)(n.a,{href:"https://docs.docker.com/compose/how-tos/model-runner/",children:"Docker Model Runner documentation"})," for more details."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"services:\n  chat:\n    build: .\n    depends_on:\n      - ai_runner\n\n  ai_runner:\n    provider:\n      type: model\n      options:\n        model: ai/mistral\n    x-defang-llm: true\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Under the hood, when you use the ",(0,a.jsx)(n.code,{children:"model"})," provider, Defang will deploy the ",(0,a.jsx)(n.strong,{children:"OpenAI Access Gateway"})," in a private network. This allows you to use the same code for both local development and cloud deployment.\nThe ",(0,a.jsx)(n.code,{children:"x-defang-llm"})," extension is used to configure the appropriate roles and permissions for your service. See the ",(0,a.jsx)(n.a,{href:"/docs/concepts/managed-llms/managed-language-models/",children:"Managed Language Models"})," page for more details."]}),"\n",(0,a.jsx)(n.h2,{id:"current-support",children:"Current Support"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Provider"}),(0,a.jsx)(n.th,{children:"Managed Language Models"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.a,{href:"/docs/providers/playground#managed-services",children:"Playground"})}),(0,a.jsx)(n.td,{children:"\u2705"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.a,{href:"/docs/providers/aws#managed-llms",children:"AWS Bedrock"})}),(0,a.jsx)(n.td,{children:"\u2705"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.a,{href:"/docs/providers/digitalocean#future-improvements",children:"DigitalOcean GenAI"})}),(0,a.jsx)(n.td,{children:"\u274c"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.a,{href:"/docs/providers/gcp#managed-llms",children:"GCP Vertex AI"})}),(0,a.jsx)(n.td,{children:"\u2705"})]})]})]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(l,{...e})}):l(e)}}}]);