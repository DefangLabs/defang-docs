"use strict";(self.webpackChunkdefang_docs=self.webpackChunkdefang_docs||[]).push([[7385],{28453:(e,n,a)=>{a.d(n,{R:()=>r,x:()=>c});var s=a(96540);const o={},t=s.createContext(o);function r(e){const n=s.useContext(t);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),s.createElement(t.Provider,{value:n},e.children)}},72179:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>p,frontMatter:()=>r,metadata:()=>s,toc:()=>i});const s=JSON.parse('{"id":"concepts/managed-llms/openai-access-gateway","title":"OpenAI Access Gateway","description":"Defang makes it easy to leverage cloud-native managed language models for your OpenAI-compatible application.","source":"@site/docs/concepts/managed-llms/openai-access-gateway.md","sourceDirName":"concepts/managed-llms","slug":"/concepts/managed-llms/openai-access-gateway","permalink":"/docs/concepts/managed-llms/openai-access-gateway","draft":false,"unlisted":false,"editUrl":"https://github.com/DefangLabs/defang-docs/tree/main/docs/concepts/managed-llms/openai-access-gateway.md","tags":[],"version":"current","frontMatter":{"title":"OpenAI Access Gateway","description":"Defang makes it easy to leverage cloud-native managed language models for your OpenAI-compatible application."},"sidebar":"conceptsSidebar","previous":{"title":"Using Managed LLMs","permalink":"/docs/concepts/managed-llms/managed-language-models"},"next":{"title":"Managed Storage","permalink":"/docs/concepts/managed-storage/"}}');var o=a(74848),t=a(28453);const r={title:"OpenAI Access Gateway",description:"Defang makes it easy to leverage cloud-native managed language models for your OpenAI-compatible application."},c="OpenAI Access Gateway",d={},i=[{value:"Docker Model Provider Services",id:"docker-model-provider-services",level:2},{value:"Model Mapping",id:"model-mapping",level:2},{value:"Current Support",id:"current-support",level:2}];function l(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"openai-access-gateway",children:"OpenAI Access Gateway"})}),"\n",(0,o.jsxs)(n.p,{children:["Defang makes it easy to deploy on your favourite cloud's managed LLM service with our ",(0,o.jsx)(n.a,{href:"https://github.com/DefangLabs/openai-access-gateway",children:"OpenAI Access Gateway"}),". This service sits between your application and the cloud service and acts as a compatibility layer.\nIt handles incoming OpenAI requests, translates those requests to the appropriate cloud-native API, handles the native response, and re-constructs an OpenAI-compatible response."]}),"\n",(0,o.jsxs)(n.p,{children:["See ",(0,o.jsx)(n.a,{href:"/docs/tutorials/deploy-openai-apps",children:"our tutorial"})," which describes how to configure the OpenAI Access Gateway for your application."]}),"\n",(0,o.jsx)(n.h2,{id:"docker-model-provider-services",children:"Docker Model Provider Services"}),"\n",(0,o.jsxs)(n.p,{children:["As of Docker Compose v2.35 and Docker Desktop v4.41, Compose introduces a new service type called ",(0,o.jsx)(n.code,{children:"provider"})," that allows you to declare platform capabilities required by your application.\nFor AI models, you use the ",(0,o.jsx)(n.code,{children:"model"})," type to declare model dependencies. This will expose an OpenAI compatible API for your service. Check the ",(0,o.jsx)(n.a,{href:"https://docs.docker.com/compose/how-tos/model-runner/",children:"Docker Model Runner documentation"})," for more details."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:"services:\n  chat:\n    build: .\n    depends_on:\n      - ai_runner\n\n  ai_runner:\n    provider:\n      type: model\n      options:\n        model: ai/mistral\n    x-defang-llm: true\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Under the hood, when you use the ",(0,o.jsx)(n.code,{children:"model"})," provider, Defang will deploy the ",(0,o.jsx)(n.strong,{children:"OpenAI Access Gateway"})," in a private network. This allows you to use the same code for both local development and cloud deployment."]}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"x-defang-llm"})," extension is used to configure the appropriate roles and permissions for your service. See the ",(0,o.jsx)(n.a,{href:"/docs/concepts/managed-llms/managed-language-models/",children:"Managed Language Models"})," page for more details."]}),"\n",(0,o.jsx)(n.h2,{id:"model-mapping",children:"Model Mapping"}),"\n",(0,o.jsxs)(n.p,{children:["Defang supports model mapping through the ",(0,o.jsx)(n.a,{href:"https://github.com/DefangLabs/openai-access-gateway",children:"openai-access-gateway"})," on AWS and GCP. This takes a model with a ",(0,o.jsx)(n.a,{href:"https://hub.docker.com/catalogs/models",children:"Docker naming convention"})," (e.g. ",(0,o.jsx)(n.code,{children:"ai/llama3.3"}),") and maps it to the closest matching model name on the target platform. If no such match can be found, it can fallback onto a known existing model (e.g. ",(0,o.jsx)(n.code,{children:"ai/mistral"}),")."]}),"\n",(0,o.jsx)(n.p,{children:"This can be configured through the following environment variables:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"USE_MODEL_MAPPING"})," (default to true) - configures whether or not model mapping should be enabled."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"FALLBACK_MODEL"})," (no default) - configure a model which will be used if model mapping fails to find a target model."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"current-support",children:"Current Support"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Provider"}),(0,o.jsx)(n.th,{children:"Managed Language Models"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.a,{href:"/docs/providers/playground#managed-services",children:"Playground"})}),(0,o.jsx)(n.td,{children:"\u2705"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.a,{href:"/docs/providers/aws#managed-llms",children:"AWS Bedrock"})}),(0,o.jsx)(n.td,{children:"\u2705"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.a,{href:"/docs/providers/digitalocean#future-improvements",children:"DigitalOcean GenAI"})}),(0,o.jsx)(n.td,{children:"\u274c"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.a,{href:"/docs/providers/gcp#managed-llms",children:"GCP Vertex AI"})}),(0,o.jsx)(n.td,{children:"\u2705"})]})]})]})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(l,{...e})}):l(e)}}}]);