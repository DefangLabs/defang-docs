"use strict";(self.webpackChunkdefang_docs=self.webpackChunkdefang_docs||[]).push([[4866],{26387:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"tutorials/deploy-openai-apps/gcp-vertex","title":"GCP Vertex AI","description":"Deploy OpenAI Apps to GCP Vertex AI using Defang.","source":"@site/docs/tutorials/deploy-openai-apps/gcp-vertex.mdx","sourceDirName":"tutorials/deploy-openai-apps","slug":"/tutorials/deploy-openai-apps/gcp-vertex","permalink":"/docs/tutorials/deploy-openai-apps/gcp-vertex","draft":false,"unlisted":false,"editUrl":"https://github.com/DefangLabs/defang-docs/tree/main/docs/tutorials/deploy-openai-apps/gcp-vertex.mdx","tags":[],"version":"current","frontMatter":{"title":"GCP Vertex AI","description":"Deploy OpenAI Apps to GCP Vertex AI using Defang."},"sidebar":"tutorialsSidebar","previous":{"title":"AWS Bedrock","permalink":"/docs/tutorials/deploy-openai-apps/aws-bedrock"},"next":{"title":"Adding Custom 1-Click Deploy to Your App","permalink":"/docs/tutorials/adding-custom-one-click-deploy"}}');var o=s(74848),i=s(28453);s(96540);const r={title:"GCP Vertex AI",description:"Deploy OpenAI Apps to GCP Vertex AI using Defang."},l="Deploy OpenAI Apps to GCP Vertex AI",a={},c=[{value:"Add an LLM Service to Your Compose File",id:"add-an-llm-service-to-your-compose-file",level:2},{value:"Notes:",id:"notes",level:3},{value:"Redirect Application Traffic",id:"redirect-application-traffic",level:2},{value:"Selecting a Model",id:"selecting-a-model",level:2},{value:"Complete Example Compose File",id:"complete-example-compose-file",level:2},{value:"Environment Variable Matrix",id:"environment-variable-matrix",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"deploy-openai-apps-to-gcp-vertex-ai",children:"Deploy OpenAI Apps to GCP Vertex AI"})}),"\n",(0,o.jsxs)(n.p,{children:["Let's assume you have an application that uses an OpenAI client library and you want to deploy it to the cloud using ",(0,o.jsx)(n.strong,{children:"GCP Vertex AI"}),"."]}),"\n",(0,o.jsxs)(n.p,{children:["This tutorial shows you how ",(0,o.jsx)(n.strong,{children:"Defang"})," makes it easy."]}),"\n",(0,o.jsx)(n.admonition,{type:"info",children:(0,o.jsxs)(n.p,{children:["You must ",(0,o.jsx)(n.a,{href:"https://cloud.google.com/vertex-ai/generative-ai/docs/control-model-access",children:"configure GCP Vertex AI model access"})," for each model you intend to use in your GCP account."]})}),"\n",(0,o.jsxs)(n.p,{children:["Suppose you start with a ",(0,o.jsx)(n.code,{children:"compose.yaml"})," file with one ",(0,o.jsx)(n.code,{children:"app"})," service, like this:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:'services:\n  app:\n    build:\n      context: .\n    ports:\n      - 3000:3000\n    environment:\n      OPENAI_API_KEY:\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:3000/"]\n'})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"add-an-llm-service-to-your-compose-file",children:"Add an LLM Service to Your Compose File"}),"\n",(0,o.jsxs)(n.p,{children:["You can use Vertex AI without changing your ",(0,o.jsx)(n.code,{children:"app"})," code by introducing a new ",(0,o.jsx)(n.a,{href:"https://github.com/DefangLabs/openai-access-gateway",children:(0,o.jsx)(n.code,{children:"defangio/openai-access-gateway"})})," service. We'll call the new service ",(0,o.jsx)(n.code,{children:"llm"}),". This new service will act as a proxy between your application and Vertex AI, and will transparently handle converting your OpenAI requests into Vertex AI requests and Vertex AI responses into OpenAI responses. This allows you to use Vertex AI with your existing OpenAI client SDK."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-diff",children:"+  llm:\n+    image: defangio/openai-access-gateway\n+    x-defang-llm: true\n+    ports:\n+      - target: 80\n+        published: 80\n+        mode: host\n+    environment:\n+      - OPENAI_API_KEY\n+      - GCP_PROJECT_ID\n+      - REGION\n"})}),"\n",(0,o.jsx)(n.h3,{id:"notes",children:"Notes:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["The container image is based on ",(0,o.jsx)(n.a,{href:"https://github.com/aws-samples/bedrock-access-gateway",children:"aws-samples/bedrock-access-gateway"}),", with enhancements."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"x-defang-llm: true"})," signals to ",(0,o.jsx)(n.strong,{children:"Defang"})," that this service should be configured to use target platform AI services."]}),"\n",(0,o.jsxs)(n.li,{children:["New environment variables:","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"REGION"})," is the zone where the services runs (e.g. ",(0,o.jsx)(n.code,{children:"us-central1"}),")"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"GCP_PROJECT_ID"})," is your project to deploy to (e.g. ",(0,o.jsx)(n.code,{children:"my-project-456789"}),")"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.admonition,{type:"tip",children:[(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"OpenAI Key"})}),(0,o.jsx)(n.p,{children:"You no longer need your original OpenAI API Key.\nWe recommend generating a random secret for authentication with the gateway:"}),(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"defang config set OPENAI_API_KEY --random\n"})})]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"redirect-application-traffic",children:"Redirect Application Traffic"}),"\n",(0,o.jsxs)(n.p,{children:["Modify your ",(0,o.jsx)(n.code,{children:"app"})," service to send API calls to the ",(0,o.jsx)(n.code,{children:"openai-access-gateway"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-diff",children:' services:\n   app:\n     ports:\n       - 3000:3000\n     environment:\n       OPENAI_API_KEY:\n+      OPENAI_BASE_URL: "http://llm/api/v1"\n     healthcheck:\n       test: ["CMD", "curl", "-f", "http://localhost:3000/"]\n'})}),"\n",(0,o.jsx)(n.p,{children:"Now, all OpenAI traffic will be routed through your gateway service and onto GCP Vertex AI."}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"selecting-a-model",children:"Selecting a Model"}),"\n",(0,o.jsx)(n.p,{children:"You should configure your application to specify the model you want to use."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-diff",children:' services:\n   app:\n     ports:\n       - 3000:3000\n     environment:\n       OPENAI_API_KEY:\n       OPENAI_BASE_URL: "http://llm/api/v1"\n+      MODEL: "google/gemini-2.5-pro-preview-03-25" # for Vertex AI\n     healthcheck:\n       test: ["CMD", "curl", "-f", "http://localhost:3000/"]\n'})}),"\n",(0,o.jsxs)(n.p,{children:["Choose the correct ",(0,o.jsx)(n.code,{children:"MODEL"})," depending on which cloud provider you are using."]}),"\n",(0,o.jsxs)(n.p,{children:["Ensure you have the necessary permissions to access the model you intend to use.\nTo do this, you can check your ",(0,o.jsx)(n.a,{href:"https://docs.aws.amazon.com/bedrock/latest/userguide/model-access-modify.html",children:"AWS Bedrock model access"})," or ",(0,o.jsx)(n.a,{href:"https://cloud.google.com/vertex-ai/generative-ai/docs/control-model-access",children:"GCP Vertex AI model access"}),"."]}),"\n",(0,o.jsxs)(n.admonition,{type:"info",children:[(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Choosing the Right Model"})}),(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["For ",(0,o.jsx)(n.strong,{children:"GCP Vertex AI"}),", use a full model path (e.g., ",(0,o.jsx)(n.code,{children:"google/gemini-2.5-pro-preview-03-25"}),"). ",(0,o.jsx)(n.a,{href:"https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/call-vertex-using-openai-library#client-setup",children:"See available Vertex AI models"}),"."]}),"\n"]})]}),"\n",(0,o.jsxs)(n.p,{children:["Alternatively, Defang supports ",(0,o.jsx)(n.a,{href:"/docs/concepts/managed-llms/openai-access-gateway/#model-mapping",children:"model mapping"})," through the ",(0,o.jsx)(n.a,{href:"https://github.com/DefangLabs/openai-access-gateway",children:"openai-access-gateway"}),". This takes a model with a Docker naming convention (e.g. ",(0,o.jsx)(n.code,{children:"ai/llama3.3"}),") and maps it to\nthe closest matching one on the target platform. If no such match can be found, it can fallback onto a known existing model (e.g. ",(0,o.jsx)(n.code,{children:"ai/mistral"}),"). These environment\nvariables are ",(0,o.jsx)(n.code,{children:"USE_MODEL_MAPPING"})," (default to true) and ",(0,o.jsx)(n.code,{children:"FALLBACK_MODEL"})," (no default), respectively."]}),"\n",(0,o.jsx)(n.h2,{id:"complete-example-compose-file",children:"Complete Example Compose File"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:'services:\n  app:\n    build:\n      context: .\n    ports:\n      - 3000:3000\n    environment:\n      OPENAI_API_KEY:\n      OPENAI_BASE_URL: "http://llm/api/v1"\n      MODEL: "google/gemini-2.5-pro-preview-03-25"\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:3000/"]\n\n  llm:\n    image: defangio/openai-access-gateway\n    x-defang-llm: true\n    ports:\n      - target: 80\n        published: 80\n        mode: host\n    environment:\n      - OPENAI_API_KEY\n      - GCP_PROJECT_ID\n      - REGION\n'})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"environment-variable-matrix",children:"Environment Variable Matrix"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Variable"}),(0,o.jsx)(n.th,{children:"GCP Vertex AI"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"GCP_PROJECT_ID"})}),(0,o.jsx)(n.td,{children:"Required"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"REGION"})}),(0,o.jsx)(n.td,{children:"Required"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"MODEL"})}),(0,o.jsxs)(n.td,{children:["Vertex model ID or Docker model name, for example ",(0,o.jsx)(n.code,{children:"publishers/meta/models/llama-3.3-70b-instruct-maas"})," or ",(0,o.jsx)(n.code,{children:"ai/llama3.3"})]})]})]})]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.p,{children:"You now have a single app that can:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Talk to ",(0,o.jsx)(n.strong,{children:"GCP Vertex AI"})]}),"\n",(0,o.jsx)(n.li,{children:"Use the same OpenAI-compatible client code"}),"\n",(0,o.jsx)(n.li,{children:"Easily switch between models or cloud providers by changing a few environment variables"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},28453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>l});var t=s(96540);const o={},i=t.createContext(o);function r(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);