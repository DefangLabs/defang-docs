---
title: Deploying your OpenAI Application to AWS Bedrock or GCP Vertex AI
sidebar_position: 50
---

# Deploying Your OpenAI Application to AWS Bedrock or GCP Vertex AI

Let's assume you have an app that uses an OpenAI client library and you want to deploy it to the cloud, either on **AWS Bedrock** or **GCP Vertex AI**.  

This tutorial shows you how **Defang** makes it easy.

Suppose you start with a compose file like this:

```yaml
services:
  app:
    build:
      context: .
    ports:
      - 3000:3000
    environment:
      OPENAI_API_KEY:
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/"]
```

---

## Add an LLM Service to Your Compose File

You need to add a new service that acts as a proxy between your app and the backend LLM provider (Bedrock or Vertex).

Add **Defang's [openai-access-gateway](https://github.com/DefangLabs/openai-access-gateway)** service:

```diff
+  llm:
+    image: defangio/openai-access-gateway
+    x-defang-llm: true
+    ports:
+      - target: 80
+        published: 80
+        mode: host
+    environment:
+      - OPENAI_API_KEY
+      - GCP_PROJECT_ID # if using GCP Vertex AI
+      - REGION
```

### Notes:

- The container image is based on [aws-samples/bedrock-access-gateway](https://github.com/aws-samples/bedrock-access-gateway), with enhancements.
- `x-defang-llm: true` signals to **Defang** that this service should be configured to use target platform AI services.
- New environment variables:
  - `REGION` is the zone where the services runs (for AWS this is equvilent of AWS_REGION)
  - `GCP_PROJECT_ID` is needed if using **Vertex AI**. (e.g.` GCP_PROJECT_ID` = my-project-456789 and `REGION` = us-central1)

:::tip
**OpenAI Key**

You no longer need your original OpenAI API Key.  
We recommend generating a random secret for authentication with the gateway:

```bash
defang config set OPENAI_API_KEY --random
```
:::

---

## Redirect Application Traffic

Modify your `app` service to send API calls to the `openai-access-gateway`:

```diff
 services:
   app:
     ports:
       - 3000:3000
     environment:
       OPENAI_API_KEY:
+      OPENAI_BASE_URL: "http://llm/api/v1"
     healthcheck:
       test: ["CMD", "curl", "-f", "http://localhost:3000/"]
```

Now, all OpenAI traffic will be routed through your gateway service and onto AWS Bedrock or GCP Vertex.

---

## Selecting a Model

You should configure your application to specify the model you want to use.

```diff
 services:
   app:
     ports:
       - 3000:3000
     environment:
       OPENAI_API_KEY:
       OPENAI_BASE_URL: "http://llm/api/v1"
+      MODEL: "anthropic.claude-3-sonnet-20240229-v1:0" # for Bedrock
+      # MODEL: "google/gemini-2.5-pro-preview-03-25" # for Vertex AI
     healthcheck:
       test: ["CMD", "curl", "-f", "http://localhost:3000/"]
```

Choose the correct `MODEL` depending on which cloud provider you are using.

Alternatively, Defang supports model mapping through the openai-access-gateway. This takes a model with a Docker naming convention (e.g. ai/lama3.3) and maps it to
the closest matching one on the target platform. If no such match can be found it can fallback onto a known existing model (e.g. ai/mistral). These environment
variables are USE_MODEL_MAPPING (default to true) and FALLBACK_MODEL (no default), respectively.

:::info
**Choosing the Right Model**

- For **AWS Bedrock**, use a Bedrock model ID (e.g., `anthropic.claude-3-sonnet-20240229-v1:0`) [See available Bedrock models](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html).
- For **GCP Vertex AI**, use a full model path (e.g., `google/gemini-2.5-pro-preview-03-25`) [See available Vertex models](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/call-vertex-using-openai-library#client-setup)

# Complete Example Compose File

```yaml
services:
  app:
    build:
      context: .
    ports:
      - 3000:3000
    environment:
      OPENAI_API_KEY:
      OPENAI_BASE_URL: "http://llm/api/v1"
      MODEL: "anthropic.claude-3-sonnet-20240229-v1:0" # or your Vertex AI model path
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/"]

  llm:
    image: defangio/openai-access-gateway
    x-defang-llm: true
    ports:
      - target: 80
        published: 80
        mode: host
    environment:
      - OPENAI_API_KEY
      - GCP_PROJECT_ID     # required if using Vertex AI
      - REGION
```

---

# Environment Variable Matrix

| Variable           | AWS Bedrock | GCP Vertex AI |
|--------------------|-------------|---------------|
| `GCP_PROJECT_ID`    | _(not used)_| Required      |
| `REGION`            | Required| Required      |
| `MODEL`             | Bedrock model ID / Docker model name | Vertex model / Docker model name |

---

You now have a single app that can:

- Talk to **AWS Bedrock** or **GCP Vertex AI**
- Use the same OpenAI-compatible client code
- Easily switch cloud providers by changing a few environment variables
:::

